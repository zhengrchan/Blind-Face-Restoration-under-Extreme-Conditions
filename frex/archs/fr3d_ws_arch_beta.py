import torch
import numpy as np
import torch.nn as nn
import torch.autograd as autograd
from basicsr.utils.registry import ARCH_REGISTRY



@ARCH_REGISTRY.register()
class StyleCodeDiscriminator(nn.Module):
    """
    Discriminator for stylecode(ws) generated by encoder
    """

    def __init__(self):
        super(StyleCodeDiscriminator, self).__init__()
        self.dis = nn.Sequential(nn.Linear(512, 128),
                                 nn.LeakyReLU(0.2, inplace=True),
                                 nn.Linear(128, 32),
                                 nn.LeakyReLU(0.2, inplace=True),
                                 nn.Linear(32, 1),
                                )

    # def compute_gradient_penalty(self, real_samples, fake_samples):
    #     """Calculates the gradient penalty loss for WGAN GP"""
    #     # Random weight term for interpolation between real and fake samples
    #     Tensor = torch.FloatTensor
    #     D = self.dis
    #     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))
    #     # Get random interpolation between real and fake samples
    #     interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
    #     d_interpolates = D(interpolates)
    #     fake = torch.Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)
    #     # Get gradient w.r.t. interpolates
    #     gradients = autograd.grad(
    #         outputs=d_interpolates,
    #         inputs=interpolates,
    #         grad_outputs=fake,
    #         create_graph=True,
    #         retain_graph=True,
    #         only_inputs=True,
    #     )[0]
    #     gradients = gradients.view(gradients.size(0), -1)
    #     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    #     return gradient_penalty

    def forward(self, x):
        return self.dis(x)
